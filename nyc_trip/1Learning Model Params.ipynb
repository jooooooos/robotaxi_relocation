{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import holidays\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.linalg import eig\n",
    "from scipy.stats import kstest\n",
    "\n",
    "from constants import (\n",
    "    MAX_TAXI_ZONE_ID,\n",
    "    location_ids,\n",
    "    excluded_location_ids,\n",
    "    location_id_to_index,\n",
    "    num_locations,\n",
    "    taxi_type,\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to go through NYC FHVHV trip data to learn:\n",
    "\n",
    "- time-varying rider arrival rates, $\\lambda_{ij}(t)$\n",
    "- time-varying trip completion rates, $\\mu_{ij}(t)$\n",
    "- rider transition matrices, $P_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = 20 # in minutes\n",
    "T_max = int(24 * (60 // Delta))\n",
    "YEARS = list(range(2019, 2025))\n",
    "\n",
    "us_holidays = holidays.US(years=YEARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $P_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"data\")\n",
    "filenames = [fn for fn in filenames if fn.startswith(taxi_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty 3D array\n",
    "# trip_counts = np.zeros((int(24 / (Delta / 60)), num_locations, num_locations), dtype=int)\n",
    "# num_dates = 0\n",
    "\n",
    "# for file in tqdm(filenames):\n",
    "#     df = pd.read_parquet(f'data/{file}')\n",
    "    \n",
    "#     unique_dates = df.pickup_datetime.dt.date.unique()\n",
    "#     working_days = [date for date in unique_dates if date.weekday() < 5 and date not in us_holidays]\n",
    "#     num_dates += len(working_days)\n",
    "    \n",
    "#     # Filter for weekdays that are NOT US holidays\n",
    "#     df = df[\n",
    "#         (df.pickup_datetime.dt.weekday < 5) &  # Monday to Friday\n",
    "#         (~df.pickup_datetime.dt.date.isin(us_holidays))  # Exclude US holidays\n",
    "#     ]\n",
    "\n",
    "#     # filter for valid locatino IDs\n",
    "#     df = df[df['PULocationID'].isin(location_ids) & df['DOLocationID'].isin(location_ids)]\n",
    "#     df['time_bin'] = (df['pickup_datetime'].dt.hour * (60 // Delta) + df['pickup_datetime'].dt.minute // Delta).astype(int)\n",
    "    \n",
    "#     # Map IDs to array indices\n",
    "#     df['pu_idx'] = df['PULocationID'].map(location_id_to_index)\n",
    "#     df['do_idx'] = df['DOLocationID'].map(location_id_to_index)\n",
    "\n",
    "#     # Group by all 3 axes\n",
    "#     grouped = df.groupby(['time_bin', 'pu_idx', 'do_idx']).size()\n",
    "\n",
    "#     # Fill in the counts using multi-index\n",
    "#     trip_counts[grouped.index.get_level_values(0),\n",
    "#                 grouped.index.get_level_values(1),\n",
    "#                 grouped.index.get_level_values(2)] += grouped.values\n",
    "\n",
    "# # Save trip_counts to a file\n",
    "# np.savez_compressed('trip_counts.npz', trip_counts=trip_counts, num_dates=num_dates)\n",
    "\n",
    "# read the trip_counts from the file\n",
    "\n",
    "with np.load('trip_counts.npz') as data:\n",
    "    trip_counts = data['trip_counts']\n",
    "    num_dates = data['num_dates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check positive recurrence of the Markov chain for each time period\n",
    "\n",
    "- Check irreducibility\n",
    "- Check positive recurrence by calculating stationary distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(11.46510580271264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_trips = np.argwhere(trip_counts == 0)\n",
    "\n",
    "len(missing_trips) / np.prod(trip_counts.shape) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of (i,j) pairs at which we don't observe ANY travels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.24837460734896633)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripcount_overalltime = trip_counts.sum(axis=0)\n",
    "missing_trips_overalltime = np.argwhere(tripcount_overalltime == 0)\n",
    "len(missing_trips_overalltime) / np.prod(tripcount_overalltime.shape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask trip_counts by 1 where 0\n",
    "\n",
    "trip_counts[trip_counts == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for t in range(T_max):\n",
    "    P = trip_counts[t] / trip_counts[t].sum(axis=1, keepdims=True)  # Normalize rows to sum to 1\n",
    "    G = nx.DiGraph(P > 0)  # adjacency matrix where there's a transition\n",
    "    is_irreducible = nx.is_strongly_connected(G)\n",
    "    if is_irreducible:\n",
    "        # solve for stationary distribution\n",
    "        \n",
    "        vals, vecs = eig(P.T, left=True, right=False)\n",
    "        i = np.argmin(np.abs(vals - 1))\n",
    "        pi = np.real(vecs[:, i])\n",
    "        pi = pi / pi.sum()  # Normalize to sum to 1\n",
    "\n",
    "        is_positive_recurrent = np.all(pi > 0)\n",
    "        result.append(is_positive_recurrent)\n",
    "    else:\n",
    "        result.append(False)\n",
    "        \n",
    "print(f\"All time bins are {'irreducible' if all(result) else 'not irreducible'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\mu_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data structure to save\n",
    "d = defaultdict(list)\n",
    "\n",
    "for file in tqdm(filenames):\n",
    "    df = pd.read_parquet(f'data/{file}')\n",
    "    \n",
    "    # Filter for weekdays that are NOT US holidays\n",
    "    df = df[\n",
    "        (df.pickup_datetime.dt.weekday < 5) &  # Monday to Friday\n",
    "        (~df.pickup_datetime.dt.date.isin(us_holidays))  # Exclude US holidays\n",
    "    ]\n",
    "\n",
    "    # filter for valid locatino IDs\n",
    "    df = df[df['PULocationID'].isin(location_ids) & df['DOLocationID'].isin(location_ids)]\n",
    "    df['time_bin'] = (df['pickup_datetime'].dt.hour * (60 // Delta) + df['pickup_datetime'].dt.minute // Delta).astype(int)\n",
    "    \n",
    "    # Map IDs to array indices\n",
    "    df['pu_idx'] = df['PULocationID'].map(location_id_to_index)\n",
    "    df['do_idx'] = df['DOLocationID'].map(location_id_to_index)\n",
    "    \n",
    "    # # groupby is too slow\n",
    "    # for (t, pu, do), group in df.groupby(['time_bin', 'pu_idx', 'do_idx']):\n",
    "    #     d[(t, pu, do)].extend(group['trip_time'].values)\n",
    "    \n",
    "    # Step 1: Create a combined key as a structured array\n",
    "    keys = df[['time_bin', 'pu_idx', 'do_idx']].to_records(index=False)\n",
    "\n",
    "    # Step 2: Sort by key to enable fast grouping\n",
    "    sorted_indices = np.argsort(keys, order=('time_bin', 'pu_idx', 'do_idx'))\n",
    "    df_sorted = df.iloc[sorted_indices].reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Create the same key array from sorted df\n",
    "    sorted_keys = df_sorted[['time_bin', 'pu_idx', 'do_idx']].to_records(index=False)\n",
    "\n",
    "    # Step 4: Find group boundaries\n",
    "    unique_keys, start_indices, counts = np.unique(sorted_keys, return_index=True, return_counts=True)\n",
    "    \n",
    "    # Step 6: Slice the trip_time column efficiently\n",
    "    trip_times = df_sorted['trip_time'].to_numpy()\n",
    "    \n",
    "    for key, start, count in zip(unique_keys, start_indices, counts):\n",
    "        d[(key.time_bin, key.pu_idx, key.do_idx)].extend(trip_times[start:start + count])\n",
    "        \n",
    "# Flatten\n",
    "keys = []\n",
    "counts = []\n",
    "trip_time_chunks = []\n",
    "\n",
    "for key, arr in tqdm(d.items()):\n",
    "    keys.append(key)\n",
    "    counts.append(len(arr))\n",
    "    trip_time_chunks.append(arr)\n",
    "\n",
    "# Concatenate all trip times into one big array\n",
    "all_trip_times = np.concatenate(trip_time_chunks)\n",
    "\n",
    "# Create key_counts array: (time_bin, pu_idx, do_idx, count)\n",
    "key_counts = np.array([\n",
    "    (*k, c) for k, c in zip(keys, counts)\n",
    "], dtype=np.int32)\n",
    "\n",
    "np.savez_compressed(\n",
    "    'all_trip_times.npz', \n",
    "    all_trip_times=all_trip_times,\n",
    "    key_counts=key_counts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    'all_trip_times.npz', \n",
    "    all_trip_times=all_trip_times,\n",
    "    key_counts=key_counts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from files\n",
    "with np.load('all_trip_times.npz') as data:\n",
    "    all_trip_times = data['all_trip_times']\n",
    "    key_counts = data['key_counts']\n",
    "    \n",
    "# Reconstruct the dictionary\n",
    "trip_times = defaultdict(list)\n",
    "start = 0\n",
    "\n",
    "for row in tqdm(key_counts):\n",
    "    t, pu, do, count = row\n",
    "    trip_times[(t, pu, do)] = all_trip_times[start:start + count] / 3600.0 # Convert to hours\n",
    "    start += count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution match of trip completion times (K-S Test, Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (time, pickup, dropoff) with the most trips\n",
    "\n",
    "lengths = [(key, len(x)) for key, x in trip_times.items() if len(x) > 0]\n",
    "lengths.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# This is 8:00am - 8:20am from Crown Heights South, Brooklyn to Crown Heights South, Brooklyn. (but why though?)\n",
    "\n",
    "travel_times = np.array(trip_times[lengths[1][0]])\n",
    "lambda_hat = 1.0 / travel_times.mean()\n",
    "D, p_value = kstest(\n",
    "    rvs=travel_times, \n",
    "    cdf='expon', \n",
    "    args=(0, lambda_hat), #  (loc, scale)\n",
    "    alternative='two-sided',\n",
    ")\n",
    "\n",
    "print(f\"p-value of K-S Test: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for travel times above 99.9 percentile and below 0.01 percentile\n",
    "# This is to remove outliers (some trips are 1 second long, some trips are like 1 hour)\n",
    "\n",
    "upper_threshold = np.quantile(travel_times, 0.999)\n",
    "lower_threshold = np.quantile(travel_times, 0.001)\n",
    "travel_times_filtered = travel_times[(travel_times > lower_threshold) & (travel_times < upper_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution mismatch between the empirical data and the fitted distribution\n",
    "\n",
    "loc, scale = scipy.stats.expon.fit(travel_times_filtered, floc=0)\n",
    "samples = np.random.exponential(scale=scale, size=10000) + loc\n",
    "plt.hist(samples, bins=100, density=True, alpha=0.5, label='Exponential PDF')\n",
    "plt.hist(travel_times_filtered, bins=100, density=True, alpha=0.5, label='Empirical PDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1: Distribution mismatch with exponential distribution\n",
    "\n",
    "Solution: Ignore it for now lmao, assume it is exponential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: Missing entries (i.e. data is still too sparse), so we don't know $\\mu_{ij}^{(t)}$ for some $i,j,t$ triplet.\n",
    "\n",
    "Patch: Use tensor completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ = np.empty((T_max, num_locations, num_locations))\n",
    "mu_[:] = np.nan\n",
    "\n",
    "for key, arr in tqdm(trip_times.items()):\n",
    "    time_bin, pu_idx, do_idx = key\n",
    "    arr = np.array(arr)\n",
    "    if arr.mean() == 0:\n",
    "        continue\n",
    "    lower_threshold = np.quantile(arr, 0.001)\n",
    "    upper_threshold = np.quantile(arr, 0.999)\n",
    "    \n",
    "    # Filter for travel times above 99.9 percentile and below 0.01 percentile\n",
    "    arr_curtailed = arr[(arr > lower_threshold) & (arr < upper_threshold)]\n",
    "    mu_[time_bin, pu_idx, do_idx] = 1 / np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. do tensor completion\n",
    "# 2. clip the tensor to the range [l_thres, u_thres] from the original tensor\n",
    "# 3. save the tensor\n",
    "\n",
    "with np.load('mu_cp.npz') as data:\n",
    "    mu_cp = data['mu']\n",
    "\n",
    "with np.load('mu.npz') as data:\n",
    "    mu_ = data['mu']\n",
    "\n",
    "u_thres = np.nanmax(mu_)\n",
    "l_thres = np.nanmin(mu_)\n",
    "\n",
    "# Clip the tensor to the range [l_thres, u_thres] from the original tensor\n",
    "mu_cp[mu_cp < l_thres] = l_thres\n",
    "mu_cp[mu_cp > u_thres] = u_thres\n",
    "\n",
    "np.savez_compressed('mu_cp_clipped.npz', mu=mu_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac, tucker, non_negative_parafac, non_negative_tucker\n",
    "from tensorly.metrics.regression import RMSE\n",
    "\n",
    "def masked_rmse(true, pred, mask):\n",
    "    return np.sqrt(np.mean((true[mask] - pred[mask]) ** 2))\n",
    "\n",
    "def rmspe(true, pred, mask, epsilon=1e-6):\n",
    "    y_true = true[mask]\n",
    "    y_pred = pred[mask]\n",
    "    return np.sqrt(np.mean(((y_true - y_pred) / (y_true + epsilon))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure backend is NumPy\n",
    "tl.set_backend('numpy')\n",
    "\n",
    "# Your tensor: shape (T, I, J), with np.nan for missing mu_ijt\n",
    "mask = ~np.isnan(mu_)  # True where observed\n",
    "\n",
    "# Replace NaN with zeros or temporary values for fitting (we'll mask later)\n",
    "filled_tensor = np.nan_to_num(mu_, nan=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CP Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 60\n",
    "\n",
    "weights, factors = non_negative_parafac(\n",
    "    filled_tensor,\n",
    "    rank=r,\n",
    "    mask=mask,\n",
    "    n_iter_max=1500,\n",
    "    init='svd'\n",
    ")\n",
    "\n",
    "mu_parafac_nn = tl.cp_to_tensor((weights, factors))\n",
    "\n",
    "print(f\"Rank {r} RMSE:\", masked_rmse(mu_, mu_parafac_nn, mask))\n",
    "print(f\"Rank {r} RMSPE:\", rmspe(mu_, mu_parafac_nn, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_thres = np.nanmax(mu_)\n",
    "l_thres = np.nanmin(mu_)\n",
    "\n",
    "# Clip the tensor to the range [l_thres, u_thres] from the original tensor\n",
    "mu_parafac_nn[mu_parafac_nn < l_thres] = l_thres\n",
    "mu_parafac_nn[mu_parafac_nn > u_thres] = u_thres\n",
    "\n",
    "# save the CP and Tucker decompositions\n",
    "np.savez_compressed('mu_cp.npz', mu=mu_parafac_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\lambda_{ij}$\n",
    "\n",
    "The number of arrivals $N_{it}^{(d)}$ on day $d$, for time bin $t$, at location $i$, is assumed to be distributed as:\n",
    "$$N_{it}^{(d)} \\sim \\text{Poisson}(\\lambda_{it}\\cdot \\Delta)$$\n",
    "\n",
    "MLE for $\\lambda_{it}$, per-hour rider arrival rate, is: $$\\hat{\\lambda}_{it} = \\frac{1}{D\\cdot (\\Delta / 60)}\\sum_{d=1}^D \\sum_{j=1}^r N_{ijt}^{(d)}$$ where $D$ is the number of observed working days, and $\\Delta$ is the time bin width in minutes, and $j$ is the destination location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = trip_counts.sum(axis=2) / (Delta / 60 * num_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if $\\lambda_{it} > 0$ for all $i$ and $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zero_lambda_indices = np.argwhere(lambda_ == 0)\n",
    "print(f\"There are {len(zero_lambda_indices)} number of location/time pair(s) with zero arrival rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
